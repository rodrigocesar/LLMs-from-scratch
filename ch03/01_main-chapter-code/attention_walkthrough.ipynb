{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b733c0a6",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Self-Attention Walkthrough (PyTorch)\n",
    "\n",
    "This notebook takes a *single simple sentence* and walks through **every step**:\n",
    "1. Tokenization → Token IDs  \n",
    "2. Token embeddings + positional embeddings  \n",
    "3. Compute **Q**, **K**, **V**  \n",
    "4. Compute attention **scores**, **weights (softmax)**  \n",
    "5. Compute the **context vectors** (attention head output)  \n",
    "6. (Optional) Extend to **multi-head attention**\n",
    "\n",
    "We keep the numbers small and deterministic for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb524c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7273c43ccf90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "torch.manual_seed(0)  # reproducible numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6281a",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Tokenization → Token IDs\n",
    "\n",
    "We'll use a **tiny whitespace tokenizer** with a small fixed vocabulary for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca12dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Time flies fast\n",
      "Tokens  : ['time', 'flies', 'fast']\n",
      "Token IDs: tensor([1, 3, 4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tiny vocab for a toy example\n",
    "vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<bos>\": 1,\n",
    "    \"<eos>\": 2,\n",
    "    \"time\": 3,\n",
    "    \"flies\": 4,\n",
    "    \"fast\": 5,\n",
    "}\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def encode(tokens):\n",
    "    # Add BOS and EOS to be explicit (optional, but educational)\n",
    "    ids = [vocab[\"<bos>\"]] + [vocab.get(t, 0) for t in tokens] + [vocab[\"<eos>\"]]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "sentence = \"Time flies fast\"\n",
    "tokens = tokenize(sentence)\n",
    "ids = encode(tokens)\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens  :\", tokens)\n",
    "print(\"Token IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b180d",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Token embeddings + positional embeddings\n",
    "\n",
    "We use small dimensions so the numbers are easy to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9be6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings:\n",
      " tensor([[ 0.1000,  0.2000,  0.3000,  0.4000],\n",
      "        [ 0.5000,  0.1000,  0.0000, -0.2000],\n",
      "        [ 0.3000, -0.1000,  0.4000,  0.1000],\n",
      "        [ 0.0500,  0.6000, -0.2000,  0.1000],\n",
      "        [ 0.2000,  0.1000, -0.1000,  0.0000]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional embeddings:\n",
      " tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0100,  0.0200,  0.0300,  0.0400],\n",
      "        [ 0.0200,  0.0100, -0.0100,  0.0000],\n",
      "        [ 0.0300,  0.0000,  0.0100, -0.0200],\n",
      "        [ 0.0400, -0.0100,  0.0200, -0.0100]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Input embeddings X (token + position):\n",
      " tensor([[ 0.1000,  0.2000,  0.3000,  0.4000],\n",
      "        [ 0.5100,  0.1200,  0.0300, -0.1600],\n",
      "        [ 0.3200, -0.0900,  0.3900,  0.1000],\n",
      "        [ 0.0800,  0.6000, -0.1900,  0.0800],\n",
      "        [ 0.2400,  0.0900, -0.0800, -0.0100]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Shape X: torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emb_dim = 4\n",
    "context_len = 8  # bigger than we need, just to show capacity\n",
    "\n",
    "tok_emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=emb_dim)\n",
    "pos_emb = nn.Embedding(num_embeddings=context_len, embedding_dim=emb_dim)\n",
    "\n",
    "# Initialize with deterministic (but non-trivial) weights for readability\n",
    "with torch.no_grad():\n",
    "    tok_emb.weight.copy_(torch.tensor([\n",
    "        [ 0.00,  0.00,  0.00,  0.00],  # <pad>\n",
    "        [ 0.10,  0.20,  0.30,  0.40],  # <bos>\n",
    "        [ 0.20,  0.10, -0.10,  0.00],  # <eos>\n",
    "        [ 0.50,  0.10,  0.00, -0.20],  # time\n",
    "        [ 0.30, -0.10,  0.40,  0.10],  # flies\n",
    "        [ 0.05,  0.60, -0.20,  0.10],  # fast\n",
    "    ]))\n",
    "    pos_emb.weight.copy_(torch.tensor([\n",
    "        [ 0.00,  0.00,  0.00,  0.00],  # position 0\n",
    "        [ 0.01,  0.02,  0.03,  0.04],\n",
    "        [ 0.02,  0.01, -0.01,  0.00],\n",
    "        [ 0.03,  0.00,  0.01, -0.02],\n",
    "        [ 0.04, -0.01,  0.02, -0.01],\n",
    "        [ 0.05,  0.02,  0.00,  0.01],\n",
    "        [ 0.06,  0.03, -0.02,  0.02],\n",
    "        [ 0.07,  0.01,  0.01,  0.03],\n",
    "    ], dtype=torch.float))\n",
    "\n",
    "# Shape: [seq_len, emb_dim]\n",
    "tok_vectors = tok_emb(ids)\n",
    "pos_vectors = pos_emb(torch.arange(len(ids)))\n",
    "x = tok_vectors + pos_vectors\n",
    "\n",
    "print(\"Token embeddings:\\n\", tok_vectors)\n",
    "print(\"\\nPositional embeddings:\\n\", pos_vectors)\n",
    "print(\"\\nInput embeddings X (token + position):\\n\", x)\n",
    "print(\"\\nShape X:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffe28c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Q, K, V (Single Head)\n",
    "\n",
    "Each token embedding is linearly projected to:\n",
    "- **Q** (Query): “What am I looking for?”  \n",
    "- **K** (Key): “What do I contain?”  \n",
    "- **V** (Value): “What can I provide?”  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4efe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[ 0.2000, -0.1000],\n",
      "        [ 0.2700,  0.1400],\n",
      "        [ 0.3550, -0.0950],\n",
      "        [-0.0550,  0.2600],\n",
      "        [ 0.0800,  0.0500]], grad_fn=<MmBackward0>)\n",
      "\n",
      "K:\n",
      " tensor([[ 0.1100,  0.2100],\n",
      "        [ 0.2010, -0.0590],\n",
      "        [ 0.2540, -0.0590],\n",
      "        [-0.0850,  0.3410],\n",
      "        [ 0.0630, -0.0040]], grad_fn=<MmBackward0>)\n",
      "\n",
      "V:\n",
      " tensor([[ 0.0700,  0.0700],\n",
      "        [ 0.1270,  0.0270],\n",
      "        [ 0.0290,  0.2150],\n",
      "        [ 0.1380, -0.2480],\n",
      "        [ 0.0950, -0.0350]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "head_dim = 2  # small for readability\n",
    "\n",
    "W_Q = nn.Linear(emb_dim, head_dim, bias=False)\n",
    "W_K = nn.Linear(emb_dim, head_dim, bias=False)\n",
    "W_V = nn.Linear(emb_dim, head_dim, bias=False)\n",
    "\n",
    "# Use deterministic weights so values are interpretable\n",
    "with torch.no_grad():\n",
    "    W_Q.weight.copy_(torch.tensor([\n",
    "        [ 0.5,  0.0,  0.5,  0.0],\n",
    "        [ 0.0,  0.5,  0.0, -0.5],\n",
    "    ]))\n",
    "    W_K.weight.copy_(torch.tensor([\n",
    "        [ 0.4, -0.1,  0.3,  0.0],\n",
    "        [-0.2,  0.6,  0.1,  0.2],\n",
    "    ]))\n",
    "    W_V.weight.copy_(torch.tensor([\n",
    "        [ 0.3,  0.1, -0.2,  0.2],\n",
    "        [ 0.1, -0.3,  0.4,  0.0],\n",
    "    ]))\n",
    "\n",
    "Q = W_Q(x)  # [seq_len, head_dim]\n",
    "K = W_K(x)  # [seq_len, head_dim]\n",
    "V = W_V(x)  # [seq_len, head_dim]\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"\\nK:\\n\", K)\n",
    "print(\"\\nV:\\n\", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd59cb",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Attention Scores → Weights → Context\n",
    "\n",
    "Scores: \\( \\text{scores} = \\frac{QK^\\top}{\\sqrt{d_k}} \\)  \n",
    "Weights: \\( \\text{softmax}(\\text{scores}) \\) row-wise  \n",
    "Context: \\( Z = \\text{weights} \\cdot V \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da5e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores (scaled):\n",
      " tensor([[ 0.0007,  0.0326,  0.0401, -0.0361,  0.0092],\n",
      "        [ 0.0418,  0.0325,  0.0427,  0.0175,  0.0116],\n",
      "        [ 0.0135,  0.0544,  0.0677, -0.0442,  0.0161],\n",
      "        [ 0.0343, -0.0187, -0.0207,  0.0660, -0.0032],\n",
      "        [ 0.0136,  0.0093,  0.0123,  0.0072,  0.0034]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention weights (softmax rows):\n",
      " tensor([[0.1982, 0.2046, 0.2062, 0.1910, 0.1999],\n",
      "        [0.2025, 0.2006, 0.2027, 0.1977, 0.1965],\n",
      "        [0.1983, 0.2065, 0.2093, 0.1871, 0.1988],\n",
      "        [0.2045, 0.1939, 0.1935, 0.2111, 0.1970],\n",
      "        [0.2009, 0.2000, 0.2006, 0.1996, 0.1989]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Context vectors Z (single head):\n",
      " tensor([[0.0912, 0.0094],\n",
      "        [0.0915, 0.0073],\n",
      "        [0.0909, 0.0111],\n",
      "        [0.0924, 0.0019],\n",
      "        [0.0917, 0.0061]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scale = math.sqrt(head_dim)\n",
    "scores = (Q @ K.T) / scale                  # [seq_len, seq_len]\n",
    "weights = torch.softmax(scores, dim=-1)     # attention distribution per query token\n",
    "Z_single = weights @ V                      # [seq_len, head_dim]\n",
    "\n",
    "print(\"Attention scores (scaled):\\n\", scores)\n",
    "print(\"\\nAttention weights (softmax rows):\\n\", weights)\n",
    "print(\"\\nContext vectors Z (single head):\\n\", Z_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a25b2",
   "metadata": {},
   "source": [
    "\n",
    "## 5) (Optional) Multi-Head Attention\n",
    "\n",
    "Multiple heads let the model learn **different types** of relationships in parallel.  \n",
    "We'll show **2 heads** and then project back to the embedding dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f3c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated heads Z (shape): torch.Size([5, 4])\n",
      "Z (multi-head) projected back to emb_dim:\n",
      " tensor([[0.0650, 0.0160, 0.0497, 0.0152],\n",
      "        [0.0653, 0.0150, 0.0501, 0.0152],\n",
      "        [0.0650, 0.0165, 0.0496, 0.0151],\n",
      "        [0.0656, 0.0130, 0.0507, 0.0153],\n",
      "        [0.0654, 0.0145, 0.0503, 0.0152]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_heads = 2\n",
    "head_dim = 2\n",
    "d_out = num_heads * head_dim  # 4\n",
    "\n",
    "# Build per-head linear layers (for clarity, explicit two-head setup)\n",
    "Q_layers = nn.ModuleList([nn.Linear(emb_dim, head_dim, bias=False) for _ in range(num_heads)])\n",
    "K_layers = nn.ModuleList([nn.Linear(emb_dim, head_dim, bias=False) for _ in range(num_heads)])\n",
    "V_layers = nn.ModuleList([nn.Linear(emb_dim, head_dim, bias=False) for _ in range(num_heads)])\n",
    "\n",
    "# Initialize head 0 same as before; head 1 slightly different\n",
    "with torch.no_grad():\n",
    "    # Head 0\n",
    "    Q_layers[0].weight.copy_(torch.tensor([[ 0.5,  0.0,  0.5,  0.0],\n",
    "                                           [ 0.0,  0.5,  0.0, -0.5]]))\n",
    "    K_layers[0].weight.copy_(torch.tensor([[ 0.4, -0.1,  0.3,  0.0],\n",
    "                                           [-0.2,  0.6,  0.1,  0.2]]))\n",
    "    V_layers[0].weight.copy_(torch.tensor([[ 0.3,  0.1, -0.2,  0.2],\n",
    "                                           [ 0.1, -0.3,  0.4,  0.0]]))\n",
    "    # Head 1 (tweaked values)\n",
    "    Q_layers[1].weight.copy_(torch.tensor([[ 0.2,  0.2,  0.4,  0.0],\n",
    "                                           [ 0.1,  0.3, -0.1,  0.3]]))\n",
    "    K_layers[1].weight.copy_(torch.tensor([[ 0.1,  0.3,  0.2, -0.1],\n",
    "                                           [ 0.2,  0.1, -0.2,  0.2]]))\n",
    "    V_layers[1].weight.copy_(torch.tensor([[ 0.2, -0.1,  0.1,  0.3],\n",
    "                                           [-0.1,  0.2,  0.0,  0.1]]))\n",
    "\n",
    "# Compute head outputs and concatenate\n",
    "head_outputs = []\n",
    "for h in range(num_heads):\n",
    "    Qh = Q_layers[h](x)\n",
    "    Kh = K_layers[h](x)\n",
    "    Vh = V_layers[h](x)\n",
    "    scores_h = (Qh @ Kh.T) / math.sqrt(head_dim)\n",
    "    weights_h = torch.softmax(scores_h, dim=-1)\n",
    "    Zh = weights_h @ Vh\n",
    "    head_outputs.append(Zh)\n",
    "\n",
    "Z_cat = torch.cat(head_outputs, dim=-1)  # [seq_len, d_out]\n",
    "\n",
    "# Output projection back to emb_dim\n",
    "out_proj = nn.Linear(d_out, emb_dim, bias=False)\n",
    "with torch.no_grad():\n",
    "    out_proj.weight.copy_(torch.tensor([\n",
    "        [ 0.5,  0.0,  0.3,  0.0],\n",
    "        [ 0.0,  0.4,  0.0,  0.6],\n",
    "        [ 0.2, -0.1,  0.5,  0.0],\n",
    "        [ 0.1,  0.0,  0.0,  0.3],\n",
    "    ]))\n",
    "\n",
    "Z_multi = out_proj(Z_cat)  # [seq_len, emb_dim]\n",
    "\n",
    "print(\"Concatenated heads Z (shape):\", Z_cat.shape)\n",
    "print(\"Z (multi-head) projected back to emb_dim:\\n\", Z_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26abba01",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Interpretability Notes\n",
    "\n",
    "- Each row of the **attention weights** corresponds to a **query token**: how much it listens to each key/token.  \n",
    "- The **context vector** for a token is a **weighted mixture** of all value vectors, using its row of weights.  \n",
    "- **Multi-head** attention lets different heads focus on different patterns; we concatenate and then project.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
